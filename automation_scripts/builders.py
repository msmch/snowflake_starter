import pandas as pd

from .basic_operations import use_db_schema, column_details, starting_row, table_name, get_primary_key, get_columns
from .file_operations import read_file


"""
Thanks to the versatility of Builder classes, I've streamlined the process of generating multiple scripts. Leveraging the fact that all staging tables 
are directly generated from corresponding files, I've automated nearly every aspect of the warehouse construction. The only manual intervention required 
pertains to optimizing the definition of staging tables. This entails adding primary and foreign key constraints, ensuring correct data types, and setting 
default values for future key generations.

I work on the repository with a meticulously standardized structure, operating under the following key assumptions:

- For staging tables, I can derive the data type of each column from the second element within each row (a row represented as a string that 
  can be conveniently split using space as a delimiter).
- My data pipeline efficiently loads data into raw_* tables within the staging schema.
- Each data ingestion event triggers a task responsible for merging data into transformed tables. These transformed tables, in turn, 
  serve as the foundation for constructing sophisticated analytics objects. 
- My transformed tables are designed to curate unique and the most up-to-date records from the staging tables. Given my reliance on data ingested from Azure storage, 
  I must anticipate the possibility of file duplication (as Snowpipe solely validates file name uniqueness).
"""


class BaseBuilder:
    """ This is the base class for other builder classes. It provides some common methods for formatting and prettifying SQL statements."""

    def __init__(self):
        self.ident = ' '*4

    def min_left_ident(self, sql: str) -> int:
        min_ident = float('inf')
        for line in sql.split('\n'):
            if line.strip():
                ident = len(line) - len(line.lstrip())
                min_ident = min(min_ident, ident)

        return min_ident

    def no_empty_row_at_top(self, sql: str) -> str:
        rows = sql.split('\n')
        for i, r in enumerate(rows):
            if len(r.strip()) > 0:  
                break

        return '\n'.join(rows[i:])      

    def shift_statement_left(self, sql: str) -> str:
        min_ident = self.min_left_ident(sql)

        return '\n'.join([line[min_ident:] for line in sql.split('\n')])

    def prettify_statement(
            self, 
            sql: str, 
            specify_db_schema: bool = False,
            **kwargs
        ) -> str:
        sql = self.no_empty_row_at_top(sql)
        sql = self.shift_statement_left(sql)

        if specify_db_schema:
            assert 'db_name' in kwargs, "'db_name' must be provided in kwargs if 'use_db_schema' is set to True"
            assert 'schema_name' in kwargs, "'db_name' must be provided in kwargs if 'use_db_schema' is set to True"
            sql = use_db_schema(kwargs['db_name'], kwargs['schema_name']) + sql

        return sql


class TableDDLBuilder(BaseBuilder):
    """
    Class that helps us build the basic table DDL. It inspects a Pandas DataFrame to determine column data types and nullable constraints.
    The file generated by this class needs some manual effort however most of the code is already there.
    """

    def __init__(self, df: pd.DataFrame):
        super().__init__()
        self.df = df

    @staticmethod
    def round_up_to_nearest(value):
        rounding_values = [10, 20, 30, 40, 50, 100, 150, 250]
        rounded_value = min(rounding_values, key=lambda x: abs(x - value) if x > value else 1000)
        if rounded_value > value:
            return rounded_value 
        else:
            return value

    def varchar_cols_length(self, offset: int = 10) -> dict:
        varchar_cols = {}
        for col in self.df.columns: 
            if self.df[col].dtype == object:
                chars = self.df[col].fillna('').apply(lambda x: len(x)).max() 
                chars = TableDDLBuilder.round_up_to_nearest(chars + offset)
                varchar_cols[col] = chars

        return varchar_cols

    def is_nullable(self, col: str) -> bool:
        if self.df[col].isnull().sum() > 0:
            return True
        else:
            return False
    
    def is_categorical(self, col: str, limit: int = 10) -> bool:
        val_counts = self.df[col].value_counts()
        if len(val_counts) <= limit:
            return True
        else:
            return False

    def data_type(self, col: str) -> str:
        dtype = self.df[col].dtype
        if dtype == object:
            return 'varchar'
        if dtype in ['int16', 'int32', 'int64']:
            return 'int'
        if dtype in ['float16', 'float32', 'float64']:
            return 'float'

    def build_statment(
            self, 
            table_name: str,
            db_name: str = 'core_db',
            schema_name: str = 'staging'
        ) -> str:
        cols_length = self.varchar_cols_length()

        sql = f'create table {table_name}(\n'
        for col in self.df.columns:
            dtype = self.data_type(col)
            if dtype == 'varchar':
                dtype = f'{dtype}({cols_length[col]})'

            optional = ''
            if not self.is_nullable(col):
                optional = ' not null'

            sql += f'    {col} {dtype}{optional},\n'

        sql += f'{self.ident}add_timestamp timestamp default current_timestamp(),\n'
        sql += f'{self.ident}edit_timestamp timestamp\n);'

        sql = self.prettify_statement(
            sql,
            specify_db_schema=True,
            db_name=db_name,
            schema_name=schema_name
        )

        return sql
    

class PipeBuilder(BaseBuilder):
    """
    This class is used for generating SQL statements to create data pipelines. It extracts column information from a SQL file  and generates 
    the necessary SQL code for data ingestion.
    """

    def __init__(self):
        super().__init__()

    def generate_base_statements(self, table_name: str, db_name: str, schema_name: str, integration_name: str = 'AZURE_NOTIFICATION', **kwargs) -> None:
        self.sql_copy = [f'copy into {table_name} (\n']
        self.sql_from = ['from (\n', f'{self.ident}select\n' ]

        file_name = table_name[4:]
        self.sql_header = f'create pipe p_{file_name}\n'
        self.sql_header += 'auto_ingest = true\n'
        self.sql_header += f"integration = '{integration_name.upper()}'\nas \n"
        self.stage = f'stg_{file_name}'

    def add_row_to_substatements(self,  i: int, row: str) -> None:
        column_name, dtype = column_details(row)
        copy_line = f'{self.ident}{column_name},\n'

        if dtype == 'date':
            from_line = f"{self.ident*2} to_date(left(stg.${i}, 10), 'YYYY-MM-DD'), {self.ident}-- {column_name}\n"
        else:
            from_line = f'{self.ident*2} stg.${i}, {self.ident}-- {column_name}\n'
            
        self.sql_copy.append(copy_line)
        self.sql_from.append(from_line)
    
    def join_sub_statements(self) -> str:
        for statement in [self.sql_copy, self.sql_from]:
            statement[-1] = statement[-1].replace(',', '')
            statement.append(')\n')

        self.sql_from[-1] = f'{self.ident}from @{self.stage} stg\n);'

        sql_copy = ''.join(self.sql_copy)
        sql_from = ''.join(self.sql_from)
        
        return self.sql_header + sql_copy + sql_from

    def generate_sql(self, content: list, first_row: int) -> str:
        rows = content[first_row + 1:]
        i = 1
        for row in rows:
            if 'skip in pipe' in row:
                continue
            if 'add_timestamp' in row: 
                break
            self.add_row_to_substatements(i, row)
            i += 1
        sql = self.join_sub_statements()

        return sql

    def build_statement(
            self,
            fpath: str, 
            db_name: str = 'core_db',
            schema_name: str = 'staging'
        ) -> str:
        content = read_file(fpath)
        first_row = starting_row(content)
        assert first_row > 0, 'CREATE TABLE clause is not included in the file.'

        table = table_name(content[first_row])
        self.generate_base_statements(
            table_name=table, 
            db_name=db_name, 
            schema_name=schema_name,
        )

        sql = self.generate_sql(content, first_row)
        sql = self.prettify_statement(
            sql,
            specify_db_schema=True,
            db_name=db_name,
            schema_name=schema_name
        )

        return sql


class StageBuilder(BaseBuilder):
    """This class is used for generating SQL statements to create data stages in Snowflake. It specifies the URL and file format for the data stage."""

    def __init__(self, url: str, file_format: str = 'csv_with_header'):
        super().__init__()
        self.url = url
        self.file_format = file_format

    def build_statement(
        self, 
        file: str, 
        db_name: str = 'core_db', 
        schema_name: str = 'staging'
        ) -> str:

        file_name = file.split('.')[0]
        target_url = f'{self.url}/{file_name}/'  

        base_sql = f"""
            create stage stg_{file_name}
            url = '{target_url}'
            storage_integration = azure_integration
            file_format = {self.file_format};
        """

        sql = self.prettify_statement(
            base_sql,
            specify_db_schema=True,
            db_name=db_name,
            schema_name=schema_name
        )
        
        return sql


class MergeBuilder(BaseBuilder):
    """
    This class is used for generating SQL statements for merging data from staging tables into transformed tables. 
    It analyzes the structure of the source file and generates SQL code for merging based on primary keys.
    """

    def __init__(self):
        super().__init__()
        self.columns = None
        self.id_col = None

    def analyze_src_file(self, fpath: str) -> None:
        """
        Variable fpath is a file path to the staging table that will be used as a source for synch with the transformed table.
        We want the latest and unique data for each primary key from the staging table. Based on that Snowflake will merge data into 
        the target table.
        """

        with open(fpath, 'r') as file:
            rows = file.readlines()
        first_row = starting_row(rows)
        self.columns = get_columns(rows, first_row)
        self.id_col = get_primary_key(rows[first_row:])

    def generate_set_str(self, columns: list, ident: str) -> str:
        return ',\n'.join([f'{ident}dest.{c} = src.{c}' for c in columns])

    def build_statement(
            self,            
            source_tbl: str, 
            target_tbl: str, 
            db_name: str = 'core_db',
            source_schema: str = 'staging', 
            target_schema: str = 'transformed',
            only_merge: bool = True
        ) -> str:
        assert self.columns is not None, 'Source file not analyzed. You need to run analyze_src_file() first.'
        
        set_str = self.generate_set_str(self.columns, self.ident*4)
        columns_str = ','.join(self.columns)
        values_str = ', '.join([f'src.{c}' for c in self.columns])

        sql = f"""
        merge into {target_schema}.{target_tbl} as dest
        using (
            select *
            from {source_tbl}
            where add_timestamp::date = current_date() or edit_timestamp::date = current_date() -- focus only on data added today to reduce computation resource needed
            qualify 1 = row_number() over (partition by {self.id_col} order by add_timestamp desc)
        ) as src
        on dest.{self.id_col} = src.{self.id_col}
        when matched then
            update set 
            {set_str}
        when not matched then
            insert ({columns_str})
            values ({values_str});
        """
        
        if only_merge:
            specify_db_schema = False
        else:
            specify_db_schema = True

        sql = self.prettify_statement(
            sql,
            specify_db_schema=specify_db_schema,
            db_name=db_name,
            schema_name=source_schema
        )

        return sql
        

class TaskBuilder(BaseBuilder):
    """
    This class is used for generating SQL statements to create Snowflake tasks. It allows you to define the SQL code that should be executed 
    by the task and set task-specific parameters such as the warehouse, frequency, and dependencies.
    """

    def __init__(self):
        super().__init__()

    def conditional_elements(self, predecesor: str, frequency: str, stream_name: str) -> str:
        after_task, schedule, where_stream_has_data = '', '', ''

        if predecesor:
            after_task = f'after {predecesor}\n'
        if frequency:
            schedule = f"schedule = '{frequency}'\n"
        if stream_name:
            where_stream_has_data = f"when system$stream_has_data('{stream_name}')\n"

        assert len(after_task) == 0 or len(schedule) == 0, "You cannot provide both 'frequency' and 'predecesor'."

        return after_task, schedule, where_stream_has_data
    
    def build_statement(
        self, 
        task_body: str,
        task_name: str, 
        warehouse: str = 'xs_backend',
        db_name: str = 'core_db',
        schema_name: str = 'staging', 
        frequency: str|None = None,
        stream_name: str|None = None, 
        predecesor: str|None = None,
        specify_db_schema: bool = False,
        ) -> str:

        after_task, schedule, where_stream_has_data = self.conditional_elements(
            predecesor,
            frequency,
            stream_name
        )

        sql = f"""
        create or replace task {task_name}
        warehouse = {warehouse}
        """
        sql = self.shift_statement_left(sql)

        for i in [schedule, after_task, where_stream_has_data]:
            if len(i)> 0:
                sql += i

        sql += f"as \n{task_body}"

        sql = self.prettify_statement(
            sql,
            specify_db_schema=specify_db_schema,
            db_name=db_name,
            schema_name=schema_name
        )

        return sql


class StreamBuilder(BaseBuilder):
    """
    This class is used for generating SQL statements to create streams on tables in Snowflake. Streams are used for real-time data replication and change tracking.
    """

    def __init__(self):
        super().__init__()
    
    def build_statement(
        self, 
        table_name: str, 
        db_name: str = 'core_db', 
        schema_name: str = 'staging'
        ):
        core = '_'.join(table_name.split('_')[1:])
        stream_name = f'stm_{core}'
        sql = use_db_schema(db_name, schema_name)
        sql += f'create stream {stream_name} on table {table_name};'

        return sql
